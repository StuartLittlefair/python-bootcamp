\documentclass[12pt,preprint]{aastex}

\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.5}
\hypersetup{colorlinks=true,linkcolor=linkcolor,citecolor=linkcolor,
            filecolor=linkcolor,urlcolor=linkcolor}
\usepackage{url}
\usepackage{amssymb,amsmath}

\newcommand{\project}[1]{{\emph{#1}}}
\newcommand{\emcee}{\project{emcee}}
\newcommand{\kepler}{\project{Kepler}}
\newcommand{\license}{MIT License}

\newcommand{\paper}{\emph{Article}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}

\newcommand{\Fig}[1]{Figure~\ref{fig:#1}}
\newcommand{\fig}[1]{\Fig{#1}}
\newcommand{\figlabel}[1]{\label{fig:#1}}
\newcommand{\Tab}[1]{Table~\ref{tab:#1}}
\newcommand{\tab}[1]{\Tab{#1}}
\newcommand{\tablabel}[1]{\label{tab:#1}}
\newcommand{\Eq}[1]{Equation~(\ref{eq:#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}
\newcommand{\Sect}[1]{Section~\ref{sect:#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\App}[1]{Appendix~\ref{sect:#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}

\newcommand{\todo}[3]{{\color{#2} \emph{#1}: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}

\newcommand{\params}{{\ensuremath{\boldsymbol{\theta}}}}
\newcommand{\data}{{\ensuremath{\boldsymbol{D}}}}
\newcommand{\hyperpar}{{\ensuremath{\alpha}}}
\newcommand{\hyperpars}{{\ensuremath{\boldsymbol{\hyperpar}}}}

\begin{document}

\title{%
Gaussian processes in astronomy: \\
An effective noise model for Kepler light curves
}

\newcommand{\nyu}{2}
\newcommand{\mpia}{3}
\author{%
    Daniel~Foreman-Mackey\altaffilmark{1,\nyu},
    David~W.~Hogg\altaffilmark{\nyu,\mpia},
    \etal
}
\altaffiltext{1}{To whom correspondence should be addressed:
                 \url{danfm@nyu.edu}}
\altaffiltext{\nyu}{Center for Cosmology and Particle Physics,
                        Department of Physics, New York University,
                        4 Washington Place, New York, NY, 10003, USA}
\altaffiltext{\mpia}{Max-Planck-Institut f\"ur Astronomie,
                        K\"onigstuhl 17, D-69117 Heidelberg, Germany}

\begin{abstract}

Correlated ``noise'' is present in most---if not all---astronomical datasets.
This noise can be due to instrumental effects---temperature or pointing
variations in the telescope, wavelength-dependent calibration issues in
spectra---or un-modeled astrophysical processes---stochastic stellar
variability when studying exoplanets.
Ignoring this source of noise will, in general, lead to biased and overly
confident inferences.
In this \paper, we describe the Gaussian process as a general, powerful
technique for taking these effects into account during data analysis.
We demonstrate the necessity and applicability of the method on both simulated
and real astronomical datasets in several fields: transiting exoplanet
discovery and characterization, stellar population modeling, and (MORE).

An open source implementation of the model described here is available at
\url{https://dan.iel.fm/george} licensed under the liberal MIT License.

\end{abstract}

\keywords{%
}

\section{Introduction}

In this \paper, we consider the problem of estimating the parameters of a
physical signal in noisy time-series data.
In particular, we are interested in the case where the amplitude of the
``noise''---this can refer to astrophysical noise (stellar variability),
systematic sensitivity variations in the detector, and the other effects
commonly considered (read noise, etc.)---in the data has an amplitude equal to
or even greater than the signal of interest.
This is a timely problem in the context of the impressive advances in
exoplanet astrophysics due to the \kepler\ mission (CITE).

Anyone who works with \kepler\ light curves is familiar with the substantial
``systematics'' in the data (CITE: PDC, Aigrain, Petigura, others).
This systematics can be due to instrumental effects (variations in
temperature, pointing, etc.) or intrinsic stellar variability.
It is common practice in the field---and astronomy in general---to build a
robust data-driven point estimate of these effects and then use the residuals
of the data away from this model as the data for the final analysis.
For example, the Presearch (sic.) Data Conditioning (PDC; and the
generalizations CITE) use the light curves of ``similar'' stars to build a
small basis of systematic light curves.
The target light curve is then fit as a linear combination of these basis
light curves and the PDC photometry are the residuals away from this fit.
There is a hope that this procedure can remove instrumental effects common
across targets while retaining astrophysical signals.
For studies of transiting exoplanets, PDC generally doesn't provide
sufficiently de-trended data (CITE) because it is specifically designed to not
substantially remove stellar variability.
This means that a second de-trending step is required before searching for
transits (CITE) or characterizing candidates (CITE).
The most common technique for this second de-trending step is a running
windowed median filter directly on the light curve (CITE).
Another procedure---commonly used when a candidate is already known---involves
masking out the candidate transits and fitting a polynomial or spline and
interpolating into the transit window.

The de-trending methods described above involve making a point estimate of the
systematic signals using the data themselves and then performing an arithmetic
operation on the data.
These operations will always introduce non-trivially correlated noise into the
data and it is very hard (impossible) to develop a procedure that will be
robust against removing the tiny signals of interest.
There is also rarely an attempt made to propagate the uncertainties introduced
by errors in the systematics model to the corrected data.
At a theoretical level, it is appealing to consider a simultaneous model of
the noise and the physical signal.
\citet{carter} studied this in the context of light curves with ``red
noise''---a very specific form of correlated noise--- by working in a wavelet
basis.
These authors found that their parameter estimates were more accurate than
estimates that ignored correlations in the noise.
\citet{gibson-gp}---and subsequently CITE Evans---have applied a more
flexible non-parametric Gaussian process (GP) model---similar to the one
described here---to the problem of modeling systematics in spectroscopic
light curves.
The wavelet method has been successfully used with \kepler\ light
curves because it scales well [$\mathcal{O}(N)$] to the large datasets by
sacrificing flexibility.
Conversely, the GP model na\"ively scales very poorly [$\mathcal{O}(N^3)$] to
larger datasets making this model largely intractable for the analysis of
 \kepler\ light curves.

Building on algorithms developed in the applied math literature (CITE Siva, et
al.), we demonstrate that a GP noise can---and in many cases, should---be
used to model real \kepler\ data.
In \sect{gps}, we describe the details of the GP model and its relation to
commonly used likelihood functions.
Then, in \sect{practical}, we discuss the practical considerations involved in
applying this model and advocate for specific algorithmic choices.
We compare the performance of our model to other industry standard techniques
when applied to signals injected into \emph{real \kepler\ light curves} in
\sect{parameters}.

\section{Parameter estimation}
\sectlabel{parameter-est}

A common data analysis problem is the estimation of a set of physical
parameters \params\ conditioned on observations \data.
Fundamentally, this procedure involves computing a \emph{likelihood function}
$p(\data\,|\,\params)$, the probability of the observations given a specific
setting of the parameters.
After we have chosen a form for the likelihood function, we can use a
non-linear optimization algorithm (gradient descent, BFGS, \etc) to find the
setting of the parameters $\params^*$ that maximizes this likelihood, the
``maximum likelihood'' result.
Alternatively, we could chose a \emph{prior} on the parameters $p(\params)$
and determine the shape of the posterior probability function
$p(\params\,|\,\data) \propto p(\data\,|\,\params)\,p(\params)$ using a
numerical technique like Markov Chain Monte Carlo (MCMC; CITE).

There is much discussion in the literature of numerical methods for solving
these inference problems (CITE) but the results will only be robust when the
likelihood function provides a reasonable description of the data.
The standard assumption is to assert that the data are independent
measurements with uncorrelated Gaussian uncertainties.
Under this assumption, the (log-)likelihood function becomes
\begin{eqnarray}\eqlabel{gauss-lnlike}
\ln p (\data\,|\,\params) &=&
-\frac{1}{2} \, \sum_{n=1}^N \left [
    \left(\frac{y_n - f_\params(\bvec{x}_n)}{{\sigma_n}^2}\right)^2
    + \log (2\,\pi\,{\sigma_n}^2)\right ] \quad.
\end{eqnarray}
where the observations $\{y_n\}_{n=1}^N$ were made at the (possibly
multidimensional) coordinates $\{x_n\}_{n=1}^N$ with Gaussian uncertainties
$\{\sigma_n\}_{n=1}^N$, and $f_\params(\bvec{x})$ is the \emph{mean model}
evaluated at the same coordinates.
This function $f_\params(\bvec{x})$ is the term that is normally referred to
as ``the model'' in astronomy and we're normally interested in learning the
parameters \params\ that specify that function.
In some other fields, the word ``model'' also refers to the chosen form of the
likelihood and prior functions.
To differentiate these two definitions, in this \paper, we will refer to the
astronomer's model as the ``mean model'' and the more general object as the
``probabilistic model''.

Recall that the key assumption in \eq{gauss-lnlike} is that noise on each
measurement is uncorrelated with the other data points.
This will, of course, only be an approximation to the true scenario because
the presence of correlated noise is ubiquitous when analyzing real data taken
using real instruments.
In many cases, this approximation is justified but in the era of precision
astronomy, there are many datasets where the probabilistic model given by
\eq{gauss-lnlike} is too rigid and the resulting inferences are precise but
biased.
In fact, there are examples in the astrophysics literature where results
improved substantially when a more flexible noise model was used (CITE).

Another important point that we will return two below is that correlated noise
can also be seen as a problem with the \emph{calibration} of the data.
\dfmtodo{Expand on this\ldots}

To motivate our discussion, let's start by considering a toy problem where the
effects of correlated noise are significant.

\paragraph{A toy problem}
For the demonstration in this section, we have generated some synthetic data
from a known mean model and added some correlated noise.
The mean model is a Gaussian feature:
\begin{eqnarray}\eqlabel{demo-base-model}
f_\bvec{\theta}(x) &=& a\,\exp\left(-\frac{[x - x_0]^2}{2\,w^2}\right)
\end{eqnarray}
where the amplitude $a$, the location $x_0$, and the width $w$ are the
parameters \params.
Since these data are synthetic, we know the true parameters
$\params_\mathrm{true}$ but we will run our inference assuming that
these are unknown and compare our results to the truth.

In the same notation as we used above, synthetic dataset has $N$ scalar
observations $\{y_n\}_{n=1}^N$ observed at input coordinates $\{x_n\}_{n=1}^N$
with uncertainties $\{\sigma_n\}_{n=1}^N$.
These given uncertainties are estimates of the \emph{independent} component of
the noise.
This component is a quantity that can often be robustly estimated using an
understanding of the measurement device, \etc\ (\dfmtodo{add some examples}).
If we look at the data plotted in \dfmtodo{SOMEFIGURE}, we can see that the
error bars don't sufficiently capture all the residuals between the model and
the data.
This is essentially a problem with the \emph{calibration} of the data and
there are a few methods that we could use for fitting the model to the data
under this effect.
The most commonly used method is to ``correct'' or ``de-trend'' the data in
advance by robustly estimating a calibration vector and transforming the
observations to account for this function.
Alternatively, we can simultaneously infer the calibration and the parameters
of the mean model $f_\params(x)$.
This second method is more conservative because the inferences include any
uncertainties in the calibration level.

Below, we will discuss two models for the calibration.
The first is a parametric technique where we choose a specific (parametric)
function for the calibration and in the second method, we model this effect
non-parametrically as structure in the correlated noise.

\paragraph{Parametric calibration}
A commonly used technique for fitting data with calibration issues is to
choose a specific (generally simple) functional form for the calibration
$q_\hyperpars(x)$ parameterized by the parameters \hyperpars.
For example, one might chose to model the long-term trends in the data as the
polynomial
\begin{eqnarray}
q_\hyperpars(x) &=& \hyperpar_0 + \hyperpar_1\,x + \hyperpar_2\,x^2 + \cdots
\quad.
\end{eqnarray}
Under this assumption, the mean model from \eq{demo-base-model} becomes
\begin{eqnarray}\eqlabel{demo-poly-model}
f_{\bvec{\theta},\bvec{\alpha}}(x) &=&
    a\,\exp\left(-\frac{[x - x_0]^2}{2\,w^2}\right)
    + q_\bvec{\alpha} (x)
\end{eqnarray}
and we can plug this function into \eq{gauss-lnlike} and run our choice of
inference procedure to find the best fit values for both \params\ and
\hyperpars.

In this toy problem, let's choose a linear model (with two parameters) for
$q_\bvec{\alpha}$ and run MCMC on the full parameter set:
\begin{eqnarray}
\{\params,\,\hyperpars\} &=& (a,\,x_0,\,w,\,\hyperpar_0,\,\hyperpar_1)
\end{eqnarray}
to get an estimate of the full marginalized posterior PDFs for the parameters
$a$, $x_0$, and $w$.
\dfmtodo{SOMEFIGURE} shows the results of this inference and the marginalized
posterior constraints on the parameters of interest.
\dfmtodo{Describe/make this figure.}

Obviously this model doesn't work in this case.

\paragraph{Correlated noise}
Instead, we can model the calibration issues as the result of correlated noise.
This model has the same number of parameters as the previous calibration model
but it is flexible enough to capture the detailed correlation structure in
noise and, as you'll see, it enables less biased inferences.
Instead of modifying the mean model as we did in \eq{demo-poly-model}, we'll
generalize the probabilistic model from \eq{gauss-lnlike}.
We'll discuss the formal motivation for this technique below but for now,
let's start by rewriting \eq{gauss-lnlike} as a matrix equation
\begin{eqnarray}\eqlabel{simple-gp-lnlike}
\ln p (\data\,|\,\params) &=&
-\frac{1}{2}\,\left[\bvec{y}-\bvec{f}_\params(\bvec{x})\right]^\mathrm{T}\,
    C^{-1} \, \left [\bvec{y} - \bvec{f}_\params(\bvec{x}) \right ]
-\frac{1}{2}\,\ln\det C - \frac{N}{2}\,\ln 2\,\pi
\end{eqnarray}
where the observations have been stacked into a vector
\begin{eqnarray}
\bvec{y} &=& \left ( \begin{array}{cccc}
y_1 & y_2 & \cdots & y_N
\end{array} \right )^\mathrm{T}
\end{eqnarray}
and similarly, for the mean model,
\begin{eqnarray}
\bvec{f}_\params(\bvec{x}) &=& \left ( \begin{array}{cccc}
f_\params(x_1) & f_\params(x_2) & \cdots & f_\params(x_N)
\end{array} \right )^\mathrm{T} \quad.
\end{eqnarray}
In \eq{simple-gp-lnlike}, the matrix $C$ is the diagonal
\begin{eqnarray}
C &=& \left (\begin{array}{cccc}
\sigma_1^2 & 0 & & 0 \\
0 & \sigma_2^2 & & 0 \\
& & \ddots & \\
0 & 0 & 0 & \sigma_N^2
\end{array}\right )
\end{eqnarray}
and, therefore,
\begin{eqnarray}
\ln\det C &=& \sum_{n=1}^{N} \ln {\sigma_n}^2 \quad.
\end{eqnarray}

Now we can incorporate correlated noise into this model by introducing some
off-diagonal elements into $C$.
If we were to allow this covariance to be completely free, we would need to
introduce $N\,(N/2-1)$ new parameters, one for each free element in the
symmetric matrix $C$.
Instead, we can choose a parametric form for the elements of the matrix so
that the covariance between the points $x_i$ and $x_j$ is given by
\begin{eqnarray}
C_{ij} &=& \delta_{ij}\,\sigma_i^2 + k_\hyperpars(x_i,\,x_j) \quad.
\end{eqnarray}
There is quite a bit of freedom in the choice of \emph{kernel function}
$k_\hyperpars(x_i,\,x_j)$ and we'll come back to this below but for now, we'll
use a commonly used function (\dfmtodo{cite RW}) called the Mat\'ern
covariance
\begin{eqnarray}
k_\hyperpars(x_i,\,x_j) &=& \dfmtodo{whatever} \quad.
\end{eqnarray}
In Figure \dfmtodo{figure}, we show the MCMC estimate of the posterior
probability marginalized over both \params\ and \hyperpars.
\dfmtodo{We rock.}


\section{The formal model}
\sectlabel{formal}

In the previous section, we introduced the concept of the Gaussian process
model by example so let's now turn to defining the model more formally.
The model is that the residuals of your data away from the mean model are
drawn from a huge $N$-dimensional Gaussian where $N$ is the number of data
points.
In other words, the dataset is a single \emph{vector} $\bvec{y}$ realization
of the Gaussian with the mean vector $\bvec{f}_\params(\bvec{x})$ and
$N \times N$ covariance matrix $K_\bvec{\alpha}$, parameterized by some
parameters \bvec{\alpha}:
\begin{eqnarray}
\bvec{y} &\sim& \mathcal{N}\left[\bvec{f}_\params(\bvec{x}),\,
    K_\bvec{\alpha}\right] \quad.
\end{eqnarray}
Therefore, the likelihood of these data under this model is the \emph{value}
of the multivariate Gaussian evaluated at \bvec{y}.
\begin{eqnarray}\eqlabel{simple-gp-lnlike-2}
\ln p (\bvec{y}\,|\,\params) &=&
-\frac{1}{2}\,\left[\bvec{y}-\bvec{f}_\params(\bvec{x})\right]^\mathrm{T}\,
{K_\bvec{\alpha}}^{-1}\,\left [\bvec{y} - \bvec{f}_\params(\bvec{x})\right ]
-\frac{1}{2}\,\ln\det K_\bvec{\alpha} - \frac{N}{2}\,\ln 2\,\pi \quad.
\end{eqnarray}


\section{Practical considerations}\sectlabel{practical}

Much of the Gaussian process literature is dedicated to computing predictions
conditioned on existing data.
In our case, and in most astrophysical contexts, we are less interested in
prediction and more interested in using the GP as a effective noise model for
data.
This means that we must invert the dense covariance matrix; and operation
that, in general, scales as the third power of the number of data points or
$\mathcal{O}(N^3)$.
Furthermore, we rarely have a reasonable \foreign{a priori} estimate for the
parameters of the covariance function so we'll want to fit for or marginalize
over these values.
This means that we also need to compute the log-determinant of the covariance
matrix.
Finally, we can't rely on our measurements being evenly spaced, making
spectral techniques unapplicable.

Due to these special constraints, we must turn to novel computational
techniques to scale GPs to datasets the size of \kepler\ light curves.


\section{Parameter estimation}\sectlabel{parameters}

\section{Discussion}\sectlabel{discussion}

\acknowledgments
SAMSI. %
It is a pleasure to thank
    Ruth Angus (Oxford),
    Tom Barclay (Ames),
    Fengji Hou (NYU),
for helpful contributions to the ideas and code presented here.
This project was partially supported by the NSF (grant AST-0908357), and NASA
(grant NNX08AJ48G).

\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\begin{thebibliography}{}\raggedright

\bibitem[Bishop(2003)]{bishop-book}
Bishop, C.~M., \emph{Pattern Recognition and Machine Learning}, Springer, 2009

\bibitem[Carter \& Winn(2009)]{carter}
Carter,~J.~A. \& Winn,~J.~N.\ 2009, \apj, 704, 51
\arxiv{0909.0747}

\bibitem[Gibson \etal(2012)]{gibson-gp}
Gibson, N.~P., Aigrain, S., Roberts, S., \etal\ 2012, \mnras, 419, 2683

\end{thebibliography}

\end{document}
